# Agent 1 - Fast BFS
We name the first agent as FastBFS, which is an agent with use of goal detection, BFS and heuristic evaluation.
To clarify the actual game status and the evaluation process, the reward is actual reward from the game engine, the score means the evaluation score generated by evaluation process that designed particularly for the game.

# Table of Contents
  * [Design Motivation](#design-motivation)
  * [Solved challenges](#solved-challenges)
 
### Design Motivation  
During preliminary stage, I explored the use of different AI techniques to implement a Sequence Agent, with the classic BFS algorithm, Q-learning, and MCTS. The main bottleneck I encountered was computational power. Based on my experience implementing these algorithms, within the 1-second time limit for computation, the available resources are only sufficient to execute fewer than 6,000 actions-equivalent to fully expanding about 120 state nodes. Considering that each level contains approximately 6×2×5 = 60 state nodes, our computational resources are insufficient to simulate even 2 full layers of actions within the time limit (i.e., one action by our side and one by the opponent).

As a result, my initial goal was to identify a suitable strategy to decompose the problem of finding the optimal solution under the current conditions into multiple sub-problems and to provide a relatively advantageous solution within the limited time. Given that Sequence is a stochastic, Partial observability information, asymmetric, zero-sum and finite game, I categorized the original problem into four sub-problems based on two dimensions: Full observability/Partial observability information and adversarial/non-adversarial settings.

|                  | Full observability Information    | Partial observability Information         |
|------------------|-----------------------------------|-------------------------------------------|
| **Opponent**     | Opponent's reward is predictable  | Opponent has high potential to get reward |
| **Agent (Self)** | Agent's own reward is predictable | Other (uncertain outcomes)                |

Based on these four sub-problems, I developed targeted strategies to prune irrelevant state branches and keep computational costs within the allowed limits:

"Opponent's getting reward is predictable":
Based on the opponent's known resources, if they are in a situation to reward ⇒ attempt to find an action that can block the opponent from scoring (Goal recognition).

"Agent's getting reward is predictable":
Based on my own resources, if I am in a position to reward ⇒ find the shortest action path that leads to scoring (BFS).

"Opponent has high potential to get reward":
If the opponent has a 3-in-a-row or 4-in-a-row ⇒ attempt to block the opponent from forming a 5-in-a-row (Goal recognition).

"Other":
Any situation that does not fall into the above categories, or where no solution exists based on current resources ⇒ find a locally optimal solution (heuristic evaluation).

Considering the interrelationship among these four situations and the engineering problems when actually calculating the local optimal solution, I have designed the following policy process:



Furthermore, since a standard action involves using one card and catching one card, in order to reduce the complexity of the problem, I regard catching a card as playing this card at the position where it can get the highest score, and the score of the card-catching behavior has been evaluated. The details of this point will be discussed in the subsequent content.

[Back to top](#table-of-contents)

### Solved Challenges
In this method, I solved the following challenges:
1. Obtain the information of the opponent's hand cards [Goal recognition](AI-Method-2.md)
2. Rate a "playing cards" behavior [Heuristic Evaluation](Heuristic-Evaluation.md)
3. Determine whether it is possible to get reward based on the currently known information [Blind Search Algorithm](AI-Method-1.md)
4. Determine whether it can block the opponent's potential scoring sequence [Goal recognition](AI-Method-2.md)
5. Find the shortest path to complete the sequence through BFS [Blind Search Algorithm](AI-Method-1.md)
6. Select action based on the score [Heuristic Evaluation](Heuristic-Evaluation.md)

* Note, here it is listed in code logics sequence. Detail of each one is listed in each page

[Back to top](#table-of-contents)
