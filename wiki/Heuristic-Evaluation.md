# Heuristic Evaluation
Heuristic Evaluation can not be strictly considered as a technique for this agent, it is listed for completeness. Any score calculation could be used to replace this calculation. However, effectiveness of calculation influence the performance a lot.

To clarify the actual game status and the evaluation process, the reward is actual reward from the game engine, the score means the evaluation score generated by evaluation process that designed particularly for the game.

# Table of Contents
  * [Motivation](#motivation)
  * [Solved challenges](#solved-challenges)
  * [Detail](#detail)
  * [Trade-offs](#trade-offs)     
     - [Advantages](#advantages)
     - [Disadvantages](#disadvantages)
  * [Future improvements](#future-improvements)
 
### Motivation
In order to choose a best action based on limit information, a heuristic evaluation score is calculated following methods below.

[Back to top](#table-of-contents)

### Solved Challenges
When there is no defence action or there is no chance that the agent can get a reward, the agent needs to choose the best action it can find.

[Back to top](#table-of-contents)

### Detail

#### Select action based on the score

Since the initial plan was to use MCTS, I designed an **Evaluator** to score the board state. Variants of this Evaluator are now used to assess the value of sub-actions such as **playing a card** and **drawing a card**. In this section, I will explain the Evaluator in detail. Later chapters will explore potential improvements to address its shortcomings, including issues of interpretability, computational cost, and its disadvantage when the agent is playing secondly.

1) Board score

Firstly, a score is assigned to each cell on the board, and then use the total score of all cells as the board score.

For each cell, its score is determined by the **minimum sequence score of all sequences that include this cell**. For each sequence, the score is calculated as:

> sequence score = number of empty cells + 2 × number of opponent tokens**

Additionally, if a cell already contains one of my own tokens, its score is increased by 1.

This scoring system is designed to reflect how many actions are minimally required to complete a reward sequence if a move is made on the given cell. 

For each scoring sequence:

* Every **empty cell** requires **1 action** (a "place").
* Every **cell occupied by an opponent** requires **2 actions** (a "remove" followed by a "place").
* If the current cell already contains one of my own tokens, placing another token on it would **waste** an action, so an additional +1 is added to its score.

For the sequence, for example, A = [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], its score (5,5,5,5,5]. 

When place in (2, 1), score is [5,4,4,4,4].

Then place in (4, 2), (4, 3), it score becomes [4,3,4,4 4]. 

Because there are three blanks in the sequence B = [(4, 0), (4, 1), (4, 2), (4, 3), (4,4)], less than the sequence in A four Spaces, so (4, 1) = 3. 

And sequence C = [(2, 1), (3, 2), (4, 3), (5, 4), (6, 5)] have 3 Spaces, so the place of the our agent (2, 1) = 3 + 1 = 4. 

If the enemy places at (6,2) at this time, the score of sequence A will stay not change because the minimum value remains unchanged. 

However, if the enemy places at (6,1), the score of sequence A will become [4,4,3,4,5], because other cells can obtain the minimum score from sequences other than sequence A, such as B. 

Then, 
> the board score = sum of each cell's sequence score.

2) Sequence score, Line score

In addition to the board score, the Evaluator also provides:

> Sequence score: indicating the number of completed reward sequences on the board.

> Line score: representing the score contributed by the most promising (i.e., nearly completed) sequences.

Since the goal during evaluation is to assign higher scores to better board states, we apply a transformation to the raw scores returned by the Evaluator. Specifically:

For both board score and line score, the score is computed as:

>  board_score = max_board_score - board_score
>  line_score = max_line_score - line_score

In this way, lower raw scores (which mean fewer actions needed to score) result in higher scores, aligning with the idea that a more favorable game state should be more highly valued.

#### Rate a "playing cards" behavior

For the Evaluator variant used to assess the "play a card" sub-action, the evaluation is based on the action itself rather than the overall board state. While the logic behind the board_score remains unchanged, an additional score based on the shortest distance to the board edge is included. This adjustment encourages to place a token closer to the center, as we observed during gameplay that central placements are generally more advantageous than edge placements.

The line_score is also modified. Instead of being computed from global board data, it now only considers the lines directly affected by the played card. This is because, when the minimum line_score on the board is 1, placing a card anywhere will not reduce the global line_score any further. As a result, a globally computed line_score becomes insensitive to the quality of the action. By focusing the evaluation on local impact, we improve the score's ability to differentiate between better and worse actions.

The Evaluator variant is also used to score the "draw a card" sub-action, as this approach is relatively simple and avoids the need for additional scoring logic.

To evaluate a draw action, the algorithm simulates playing the drawn card in its best possible position—since each card can typically be played in two possible spots. The resulting score is then discounted, because in actual gameplay, the drawn card cannot be played until the agent's next turn.

For evaluating special cards such as one-eyed Jacks and two-eyed Jacks, I use fixed empirical values. Based on the current board state, these cards are assigned a fixed bonus score, reflecting their generally high strategic value without requiring a full simulation.

Finally, the final score is 
> score = SEQUENCE_WEIGHT * sequence_score ** SEQUENCE_EXPONENT + LINE_WEIGHT * line_score ** LINE_EXPONENT + BOARD_WEIGHT * board_score ** BOARD_EXPONENT

The parameters now are: score = 100 * sequence_score ** 2 + 1 * line_score ** 2 + 1 * board_score ** 1

Since completing a sequence is the main objective of the game, it is assigned a very high score. Once the number of completed sequences exceeds 1, the game ends immediately—so it's critical for the agent to prioritize these scenarios. To emphasize this, the sequence_score is squared in the score formula, further amplifying its impact.

For incomplete sequences, a 4-in-a-row is clearly more valuable than a 3-in-a-row. To reflect this strategic difference, the line_score is also squared, giving higher weight to amplifying its impact on the board.


[Back to top](#table-of-contents)

### Trade-offs
#### *Advantages*
Choose the card and places combination that are most beneficial to the agent.

#### *Disadvantages*
The computation logic is hard to explain, and is defined based on empirical values, the computation is also costly. 
When the agent is playing secondly, it will try to escape from the other agent. Which means, if the opponent occupies the centre area, the agent will go for the area around, which are not beneficial area. These means, the agent will lose more when it plays secondly. 

[Back to top](#table-of-contents)

### Future improvements
It would be better if a simple in logic but at the same time meaningful evaluation method can be found, to simply the calculation process.
it will also improve the agent if the second-move disadvantage could be resolved. 

[Back to top](#table-of-contents)
